# -*- coding: utf-8 -*-
"""Loan Approval Prediction System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qd7Vam8lJL-IdyfngbA6wmm6Tun4QQlX

Goal: Build a complete ML solution from data cleaning to prediction.

> 1: Importing Libraries
"""

# import libraries for data manipulation and anlysis
import pandas as pd
import numpy as np

# import libraries for preprocessing and modeling
from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
#import smote for streamlit support
from imblearn.over_sampling import SMOTE

# import librariea for evaluation
from sklearn.metrics import accuracy_score,f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# import libraries for visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Set plotting style for professional visualizations
sns.set_style("whitegrid")
plt.rcParams.update({
    'font.size': 10,
    'axes.titlesize': 12,
    'axes.labelsize': 10,
    'legend.fontsize': 9,
    'axes.titleweight': 'bold'
})

"""-----------------------------------------
----------------------------------------

> 2: Loading Data
"""

# Load training and test datasets with error handling
try:
    train_data = pd.read_csv("/content/train_u6lujuX_CVtuZ9i.csv")
    test_data = pd.read_csv("/content/test_Y3wMUE5_7gLdaTN.csv")
    print("Files loaded successfully.")
except FileNotFoundError as e:
    print(f"File not found: {e.filename}. Please check the file name and path.")
    exit()
except pd.errors.ParserError:
    print("Error parsing the CSV file. Please check for formatting issues.")
    exit()
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    exit()

"""---------------------------------------------
--------------------------------------------

> 3: Data Understanding

# 3.1 Data head
"""

train_data.head()

"""# 3.2 columns names"""

train_data.columns

"""# 3.3 shape of data"""

train_data.shape

"""# 3.4 info of data"""

train_data.info()

"""# 3.5 data types"""

train_data.dtypes

"""# 3.6  Check null values"""

train_data.isnull().sum()

train_data.describe()

"""# 3.7 Target variable distribution (train only)"""

# 3.7 Target variable distribution
print(f"\n Target Variable Distribution (Training Data):")
target_counts = train_data['Loan_Status'].value_counts()
print(target_counts)
print(f"Approval Rate: {target_counts['Y']/len(train_data)*100:.1f}%")

"""----------------------------------------
-----------------------------------------

> 4:  Exploratory Data Analysis (EDA)

# 4.1 Target distribution
"""

#check how many rejected and how many approved
train_data['Loan_Status'].value_counts()

print("\nLoan Status Distribution:")
print(train_data['Loan_Status'].value_counts())
plt.figure(figsize=(7, 5))
sns.countplot(x='Loan_Status', data=train_data, palette=['blue', 'orange'])
plt.title('Loan Approval Distribution')
plt.xlabel('Loan Status (N=Rejected, Y=Approved)')
plt.ylabel('Count')
plt.legend(title='Loan Status', labels=['Rejected', 'Approved'])
plt.show()

"""# 4.2 Gender vs Loan Status"""

# Groupby gender and loan status
train_data.groupby(['Gender', 'Loan_Status']).size().unstack()

plt.figure(figsize=(7, 5))
sns.countplot(x='Gender', hue='Loan_Status', data=train_data, palette=['teal', 'coral'])
plt.title('Gender vs Loan Status')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.legend(title='Loan Status', labels=['Approved', 'Rejected'])
plt.show()

"""# 4.3 Married vs Loan Status

"""

#grupby married and loan status
train_data.groupby(['Married', 'Loan_Status']).size().unstack()

# Married vs Loan Status
plt.figure(figsize=(7, 5))
sns.countplot(x='Married', hue='Loan_Status', data=train_data, palette=['navy', 'gold'])
plt.title('Married vs Loan Status')
plt.xlabel('Married')
plt.ylabel('Count')
plt.legend(title='Loan Status', labels=['Approved', 'Rejected'])
plt.show()

"""# 4.4 Income distribution"""

plt.figure(figsize=(8, 6))
sns.histplot(train_data['ApplicantIncome'], bins=30, kde=True, color='coral')
plt.title('Applicant Income Distribution')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

"""# 4.5 Loan Amount distribution"""

plt.figure(figsize=(8, 6))
sns.histplot(train_data['LoanAmount'].dropna(), bins=50, kde=True, color='navy')
plt.title('Loan Amount Distribution')
plt.xlabel('Loan Amount')
plt.ylabel('Frequency')
plt.show()

"""# 4.6 Credit History vs Loan Status"""

#groupby credit history and loan status
train_data.groupby(['Credit_History', 'Loan_Status']).size().unstack()

plt.figure(figsize=(7, 5))
sns.countplot(x='Credit_History', hue='Loan_Status', data=train_data, palette=['purple', 'green'])
plt.title('Credit History vs Loan Status')
plt.xlabel('Credit History')
plt.ylabel('Count')
plt.legend(title='Loan Status', labels=['Rejected', 'Approved'])
plt.show()

"""# 4.7 Dependents vs Loan Status"""

#groupby dependents and loan status
train_data.groupby(['Dependents', 'Loan_Status']).size().unstack()

plt.figure(figsize=(7, 5))
sns.countplot(x='Dependents', hue='Loan_Status', data=train_data, palette=['blue', 'orange'])
plt.title('Dependents vs Loan Status')
plt.xlabel('Dependents')
plt.ylabel('Count')
plt.legend(title='Loan Status', labels=['Approved', 'Rejected'])
plt.show()

"""#Approval rates by category

# 4.8 Correlation heatmap
"""

numerical_cols = train_data.select_dtypes(include=[np.number]).columns
plt.figure(figsize=(9, 7))
correlation_matrix = train_data[numerical_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix of Numerical Features')
plt.tight_layout()
plt.show()

"""------------------
----------------

> # 5  Data Preprocessing

#5.1 Feature engineering
"""

# Create new feature( Total)income )
train_data['total_income'] = train_data['ApplicantIncome'] + train_data['CoapplicantIncome']
# Log transform to handle skewness
train_data['log_total_income'] = np.log1p(train_data['total_income'])
# Plot
plt.figure(figsize=(6, 6))
sns.histplot(train_data['log_total_income'], bins=20, kde=True, color='navy')

# Create new feature log loan amount
train_data['log_loan_amount'] = np.log1p(train_data['LoanAmount'])
# Plot
plt.figure(figsize=(6, 6))
sns.histplot(train_data['log_loan_amount'], bins=20, kde=True, color='red')

# create new feature
train_data['loan_to_income_ratio'] = train_data['LoanAmount'] / train_data['total_income']
#plot
plt.figure(figsize=(6, 6))
sns.histplot(train_data['loan_to_income_ratio'], bins=20, kde=True, color='green')
plt.title('Loan to Income Ratio Distribution')
plt.xlabel(' Income Ratio')
plt.ylabel('Frequency')
plt.show()

"""# 5.2 Apply to test data"""

# Combine applicant and co-applicant incomes
test_data['total_income'] = test_data['ApplicantIncome'] + test_data['CoapplicantIncome']

# Apply log transformation to reduce skewness in income distribution
test_data['log_total_income'] = np.log1p(test_data['total_income'])

# Apply log transformation to the loan amount to reduce skewness
test_data['log_loan_amount'] = np.log1p(test_data['LoanAmount'])

# Calculate the loan-to-income ratio to understand the debt
test_data['loan_to_income_ratio'] = test_data['LoanAmount'] / test_data['total_income']

"""# 5.3 Handle Outliers using IQR"""

# outliers handling
def clip_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[column] = np.clip(df[column], lower_bound, upper_bound)
    return df

for col in ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']:
    train_data = clip_outliers(train_data, col)
    test_data = clip_outliers(test_data, col)

# Set up the plotting area
plt.figure(figsize=(13, 6))  # Wide figure for 3 side-by-side plots

# List of columns to visualize
cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']

for i, col in enumerate(cols):
    plt.subplot(1, 3, i+1)  # Create subplot for each column
    sns.boxplot(data=[train_data[col], test_data[col]], palette='pastel')
    plt.xticks([0, 1], ['Train', 'Test'])
    plt.title(f'{col} (Outliers Clipped)')
    plt.ylabel('Value')

plt.tight_layout()
plt.show()

"""# 5.4 Handling missing values

# Train data
"""

# Fill missing values in categorical columns with mode
# Mode is suitable for categorical variables because it replaces missing values with the most common category
for col in ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History', 'Loan_Amount_Term']:
    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])

# Fill missing values in LoanAmount with the median
# Median is used for numerical features
train_data['LoanAmount'] = train_data['LoanAmount'].fillna(train_data['LoanAmount'].median())

# Fill missing values in log-transformed total income using the median
train_data['log_total_income'] = train_data['log_total_income'].fillna(train_data['log_total_income'].median())

# Fill missing values in log-transformed loan amount using the median
train_data['log_loan_amount'] = train_data['log_loan_amount'].fillna(train_data['log_loan_amount'].median())

# Fill missing values in loan-to-income ratio using the median
train_data['loan_to_income_ratio'] = train_data['loan_to_income_ratio'].fillna(train_data['loan_to_income_ratio'].median())

#check null values in train data
train_data.isnull().sum()

"""#Test data"""

# Fill missing values in categorical columns with mode
# Mode is for categorical features
for col in ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History', 'Loan_Amount_Term']:
    test_data[col] = test_data[col].fillna(test_data[col].mode()[0])

# Fill missing values in LoanAmount with the median
test_data['LoanAmount'] = test_data['LoanAmount'].fillna(test_data['LoanAmount'].median())

# Fill missing values in log-transformed total income with the median
test_data['log_total_income'] = test_data['log_total_income'].fillna(test_data['log_total_income'].median())

# Fill missing values in log-transformed loan amount using the median
test_data['log_loan_amount'] = test_data['log_loan_amount'].fillna(test_data['log_loan_amount'].median())

# Fill missing values in the loan-to-income ratio using the median
test_data['loan_to_income_ratio'] = test_data['loan_to_income_ratio'].fillna(test_data['loan_to_income_ratio'].median())

#check null values in test data
test_data.isnull().sum()

"""# 5.5 Encode categorical variables"""

# 5.5 Encode Categorical Variables

# Define the categorical columns to encode
categorical_cols = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area']

# Dictionary to store fitted LabelEncoders for later use or inverse transform
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()

    # Combine train and test column data to ensure consistent encoding
    combined_data = pd.concat([train_data[col].astype(str), test_data[col].astype(str)], axis=0)

    # Fit LabelEncoder on combined data
    le.fit(combined_data)

    # Transform both train and test data using the same encoder
    train_data[col] = le.transform(train_data[col].astype(str))
    test_data[col] = le.transform(test_data[col].astype(str))

# Save the encoder to the dictionary
#import pickle
    import pickle
    label_encoders[col] = le
    with open(f'label_encoder_{col}.pkl', 'wb') as f:
        pickle.dump(le, f)

# Encode the target variable Loan_Status using a separate LabelEncoder
le_target = LabelEncoder()
train_data['Loan_Status'] = le_target.fit_transform(train_data['Loan_Status'])

# Save the target LabelEncoder for future decoding
with open('label_encoder_Loan_Status.pkl', 'wb') as f:
    pickle.dump(le_target, f)

"""# 5.6 Data Splitting"""

X = train_data.drop(['Loan_ID', 'Loan_Status'], axis=1)
y = train_data['Loan_Status']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X.shape

y.shape

"""# 5.7 Scale features"""

# Ensure test_data has only the features used in training
# Make an explicit copy to avoid warnings
test_data_features = test_data[X_train.columns].copy()

# Apply clipping
for col in test_data_features.columns:
    if col in ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term',
               'total_income', 'log_total_income', 'log_loan_amount', 'loan_to_income_ratio']:
        lower = train_data[col].quantile(0.01)
        upper = train_data[col].quantile(0.99)
        test_data_features[col] = np.clip(test_data[col], lower, upper)

# scale
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
test_data_scaled = scaler.transform(test_data_features)

import joblib
# Recreate and save scaler
scaler = StandardScaler()
# Fit scaler on data
scaler.fit(X_train)
joblib.dump(scaler, "scaler.pkl")

"""------------------------------------------------------
------------------------------------------------------

> # 6 Model Training

# 6.1 Define Models
"""

models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'SVM': SVC(random_state=42, probability=True)
}

"""# 6.2 Hyperparameter grids"""

# Hyperparameter grids
param_grids = {
    'Random Forest': {
        'n_estimators': [100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5]
    },
    'Gradient Boosting': {
        'n_estimators': [100, 200],
        'learning_rate': [0.01, 0.1],
        'max_depth': [3, 5]
    },
    'Logistic Regression': {
        'C': [0.1, 1, 10],
        'solver': ['liblinear', 'lbfgs']
    },
    'SVM': {
        'C': [0.1, 1, 10],
        'kernel': ['rbf', 'linear']
    }
}

"""# 6.3 Train, Evaluate, and Select Best Model"""

# Initialize  variables for model accuracies and best model selection
model_accuracies = {}
best_accuracy = 0
best_model = None
best_model_name = ""

#Train each model using GridSearchCV with cross-validation
for name, model in models.items():
    print(f"\nTraining {name}...")
    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train_scaled, y_train)

# Best model from grid search
    best_estimator = grid_search.best_estimator_
    y_pred = best_estimator.predict(X_val_scaled)
    accuracy = accuracy_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    roc_auc = roc_auc_score(y_val, best_estimator.predict_proba(X_val_scaled)[:, 1])

##Store and display the model's evaluation metrics and best parameters
    model_accuracies[name] = accuracy
    print(f"{name} Best Parameters: {grid_search.best_params_}")
    print(f"Validation Accuracy: {accuracy:.4f}")
    print(f"Validation F1-Score: {f1:.4f}")
    print(f"Validation ROC-AUC: {roc_auc:.4f}")
    print(f"Classification Report:\n{classification_report(y_val, y_pred)}")

# Confusion Matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = best_estimator
        best_model_name = name

"""# 6.4 Best Model Evaluation"""

#import joblib
import joblib
print(f"\nModel Accuracies:")
for name, acc in model_accuracies.items():
    print(f"{name}: {acc:.4f}")
print(f"\nBest Model: {best_model_name}")
print(f"Best Validation Accuracy: {best_accuracy:.4f}")

"""# 6.5 Comparing  models using a leaderboard"""

# Initialize lists to store metrics
model_names = []
accuracies = []
f1_scores = []
roc_aucs = []

# Collect metrics from GridSearchCV results
for name, model in models.items():
    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train_scaled, y_train)
    best_estimator = grid_search.best_estimator_
    y_pred = best_estimator.predict(X_val_scaled)
    model_names.append(name)
    accuracies.append(accuracy_score(y_val, y_pred))
    f1_scores.append(f1_score(y_val, y_pred))
    roc_aucs.append(roc_auc_score(y_val, best_estimator.predict_proba(X_val_scaled)[:, 1]))

# Create a DataFrame for the leaderboard
leaderboard = pd.DataFrame({
    'Model': model_names,
    'Accuracy': accuracies,
    'F1-Score': f1_scores,
    'ROC-AUC': roc_aucs
})

# Sort by Accuracy
leaderboard = leaderboard.sort_values(by='Accuracy', ascending=False)

# Display the leaderboard
print("\nModel Leaderboard:")
print(leaderboard)

# Visualize the leaderboard
plt.figure(figsize=(10, 6))
# Melt the DataFrame to have metrics as a single column for plotting
leaderboard_melted = leaderboard.melt(id_vars='Model', value_vars=['Accuracy', 'F1-Score', 'ROC-AUC'],
                                     var_name='Metric', value_name='Score')
sns.barplot(x='Model', y='Score', hue='Metric', data=leaderboard_melted, palette='viridis')
plt.title('Model Performance Leaderboard')
plt.xlabel('Model')
plt.ylabel('Score')
plt.legend(title='Metric')
plt.tight_layout()
plt.show()

"""# 6.5 Save Best Model

"""

joblib.dump(best_model, f'{best_model_name}_model.pkl')
print(f"\nBest model saved as '{best_model_name}_model.pkl'")

# When training your model, save feature columns:
joblib.dump(list(X_train.columns), "columns.pkl")

"""----------------------
---------------------

#7 Feature importance
"""

# If best model is Random Forest, use it directly
if best_model_name == 'Random Forest':
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': best_model.feature_importances_
    })
# train a separate Random Forest just for feature importance
else:
    rf = RandomForestClassifier(random_state=42)
    rf.fit(X_train_scaled, y_train)
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': rf.feature_importances_
    })

# Sort and display
feature_importance = feature_importance.sort_values('Importance', ascending=False)
print("\nFeature Importance (Random Forest)")
print(feature_importance)

# Plot
plt.figure(figsize=(8, 6))
sns.barplot(x='Importance', y='Feature',hue='Importance', data=feature_importance, palette='viridis')
plt.title('Feature Importance (Random Forest)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

"""#8  ROC curve plots"""

plt.figure(figsize=(7, 5))
for name, model in models.items():
    y_prob = grid_search.best_estimator_.predict_proba(X_val_scaled)[:, 1]
    fpr, tpr, _ = roc_curve(y_val, y_prob)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_val, y_prob):.4f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curves')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

"""#9 Test predictions"""

# Fit label encoder on the original training labels
label_encoder = LabelEncoder()
label_encoder.fit(train_data['Loan_Status'])

# Predict
test_predictions = best_model.predict(test_data_scaled)

# Inverse transform: convert 0/1 back to Y/N
test_data['Loan_Status'] = label_encoder.inverse_transform(test_predictions)

# Save to CSV
test_data[['Loan_ID', 'Loan_Status']].to_csv('test_predictions.csv', index=False)
print("Test predictions saved to 'test_predictions.csv'")

